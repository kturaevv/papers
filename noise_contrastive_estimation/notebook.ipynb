{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise-contrastive estimation: A new estimation principle for unnormalized statistical models\n",
    "---\n",
    "Guntmann states that say we want to model some complex solution using statistical models. For high dimensionality problems it is close to impossible to calculate them. And to calculate them we need to compute the integral of the Probability Density Function (PDF). Even if we manage to find the the PDF it is hard to impossible to find its integral due to complexity.\n",
    "\n",
    "One of examples is using Energy Based models or multi-layer networks, which reference Neural Networks. They can be considered as one of the examples of PDF that can solve and model very complex tasks. It is both impossible to compute the integral of the Network because of its high complexity and computationally very expensive as well, say by using Softmax, given large number of classes present in the dataset as calculating softmax can be considered a form of finding Z, specifically for discrete and finite scenarios. \n",
    "\n",
    "The softmax function computes the normalization factor directly in its denominator (which sums up the exponentials of all outputs) to ensure the outputs form a valid probability distribution. This sum effectively acts as Z by normalizing the values so that their total is 1. So, in natual language processing or image detection with a lot of classes it quickly becomes infeasible / ineffective to compute directly. NCE is to eliminate the computational cost of softmax normalization.\n",
    "\n",
    "Paper proposes new approach to solve the problem. Here are a few key points:\n",
    "\n",
    "-   > The basic idea is to estimate the parameters by learning to discriminate between the data x and some artificially generated noise y.\n",
    "\n",
    "-   > ... the noise distribution should be close to the data distribution ...\n",
    "\n",
    "-   > ... one could choose a noise distribution by first estimating a preliminary model of the data, and then use this preliminary model as the noise distribution.\n",
    "  \n",
    "-   > ... we include the normalization constant as another parameter c of the model.\n",
    "\n",
    "-   > The parameters Î¸ are then estimated by learning to discriminate between the data x and the noise y, i.e. by maximizing J(T) ...\n",
    "\n",
    "The cost function $J$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_T(\\theta) &= \\frac{1}{2T} \\sum_{t} \\left( \\ln [ h(\\mathbf{x}_t; \\theta) ] + \\ln [ 1 - h(\\mathbf{y}_t; \\theta) ] \\right) \\\\\n",
    "\\text{where} \\\\\n",
    "h(u; \\theta) &= \\frac{1}{1 + \\exp[-G(u; \\theta)]} \\\\\n",
    "G(u; \\theta) &= \\ln p_m(u; \\theta) - \\ln p_n(u)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
