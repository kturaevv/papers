{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet Classification with Deep Convolutional Neural Networks\n",
    "---\n",
    "\n",
    "This seminal paper demonstrated the practical effectiveness of deep Convolutional Neural Networks (CNNs) by winning the 2012 ImageNet challenge (ILSVRC) and sparked a new wave of interest in AI. Most notable was the dramatic reduction in error rates achieved through a deep architecture of 5 convolutional layers and 3 fully connected layers, powered by 2 GPUs that not only shared the load but specialized in learning different features.\n",
    "\n",
    "They effectively showed how to address key challenges in training deep networks, including:\n",
    "\n",
    "- usage of ReLU activation function against Tanh which proved to converge 6x faster\n",
    "- usage of Local Response Normalization (LRN) layer, which was important before batch normalization existed\n",
    "- usage of two GPUs for training one big model with split architecture\n",
    "- usage of dropout layers to avoid overfitting on the large dataset\n",
    "- usage of overlapping pooling layers for improved performance\n",
    "- usage of computationally free data augmentation techniques\n",
    "\n",
    "The paper's success transformed computer vision by proving deep CNNs could work in practice on large-scale datasets, setting the foundation for modern deep learning and the trend toward even deeper architectures in following years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 227, 227])\n",
      "Output shape: torch.Size([1, 1000])\n",
      "Number of model parameters: 60965224\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2, groups=2), # split input into 2 GPU groups\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 5\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # Layer 6\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 7\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 8 (output)\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize model weights as described in the paper:\n",
    "        - Conv layers: drawn from N(0, 0.01)\n",
    "        - Bias terms: initialized with 1 for conv layers 2, 4, 5\n",
    "        - Bias terms: initialized with 0 for remaining layers\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    if m in [self.features[4], self.features[10], self.features[12]]:  # layers 2, 4, 5\n",
    "                        nn.init.constant_(m.bias, 1)\n",
    "                    else:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = AlexNet(num_classes=1000)\n",
    "x = torch.randn(1, 3, 227, 227)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
