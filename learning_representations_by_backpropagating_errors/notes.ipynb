{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning representations by back-propagating errors\n",
    "\n",
    "---\n",
    "**Quick summary:** *paper that introduced and popularized the concept of backpropagation for Neural Networks*\n",
    "\n",
    "> The task is specified by giving the desired state vector of the output units for each state vector of the input units\n",
    "\n",
    "Quite convoluted way of saying that to make a network train we need to provide it the intput data with corresponding output we hope to achieve, i.e. a supervised learning. In other words, to map inputs to the outputs with the data.\n",
    "\n",
    "> ... state of the units in each layer are determined by applying equations (1) and (2) ...\n",
    "\n",
    "Description of how Neural Networks work, i.e. every output of a layer should correspond to every input node of next layer. And layers are pieced together by a linking linear function with non-linearity.\n",
    "\n",
    "A Unit consist of 2 values which is a **weight** and a **bias**. In a modern terminology this is reffered to as a *Neuron*\n",
    "\n",
    "> The total input, $x_{j}$ to unit j is a linear function of the outputs,\n",
    "> $y_{i}$ of the units that are connected to j and of the weights, $w_{ji}$\n",
    "> on these connections $ x_{j} = \\sum_{i}y_{i}w_{ji} $\n",
    "\n",
    "... the states of the \"units\", aka Neurons, are evaluated by the linear relation of inputs, which means that an output of neuron $x_{j}$, is a sum of weights of every input to every weight of the Neuron:\n",
    "\n",
    "```python\n",
    "    def __call__(self, y: \"Units\") -> \"Units\":\n",
    "        x_j = sum(y_i * w_ji for y_i, w_ji in zip(y.w, self.w))\n",
    "        return x_j\n",
    "```\n",
    "\n",
    "> A unit has a real-valued output, $y_{j}$ which is a non-linear function of its total input $ y_{j} = {1 \\over 1 + e^{-x_{j}} }$\n",
    "\n",
    "This is the non-linearity that makes the network learn stuff, which is commonly reffered to as *activation function*. The one mentioned in the paper is *Sigmoid activation function*.\n",
    "\n",
    "```python\n",
    "    def sigmoid(self) -> \"Scalar\":\n",
    "        return 1 / (1 + math.exp(-self.w))\n",
    "\n",
    "    def __call__(self, y: \"Units\") -> \"Units\":\n",
    "        x_j = sum(y_i * w_ji for y_i, w_ji in zip(y.w, self.w))\n",
    "        out = x_j.sigmoid() # add non-linearity\n",
    "        return out\n",
    "```\n",
    "\n",
    "Based on above mentioned ideas it is enough to \"build\" the basic implementation for a simplest, non-optimized neural network, featuring:\n",
    "    - Scalar: the most basic building block of a Neural network\n",
    "    - Units: aka Neuron\n",
    "    - State Vector: aka Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30180932633148394]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Scalar:\n",
    "    \n",
    "    def __init__(self, data = None) -> None:\n",
    "        self.w: float = data if data is not None else random.uniform(-1, 1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Scalar(w={self.w})\"\n",
    "\n",
    "    def __add__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "\n",
    "        self.w += other.w\n",
    "        return self\n",
    "\n",
    "    def __mul__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "\n",
    "        self.w *= other.w\n",
    "        return self\n",
    "\n",
    "    def __pow__(self, power: int):\n",
    "        if isinstance(power, Scalar):\n",
    "            power = power.w\n",
    "        self.w ^= power\n",
    "        return self\n",
    "    \n",
    "    \"\"\" Handle other use cases using base operations. \"\"\"\n",
    "\n",
    "    def __neg__(self) -> \"Scalar\":  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other: \"Scalar\") -> \"Scalar\":  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other: \"Scalar\") -> \"Scalar\":  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other: \"Scalar\") -> \"Scalar\":  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other: \"Scalar\") -> \"Scalar\":  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other: \"Scalar\") -> \"Scalar\":  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other: \"Scalar\") -> \"Scalar\":  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def sigmoid(self) -> \"Scalar\":\n",
    "        return 1 / (1 + math.exp(-self.w))\n",
    "\n",
    "class Units: # aka a single Neuron\n",
    "    \n",
    "    def __init__(self, n_in: int) -> None:\n",
    "        self.w: list[Scalar] = [Scalar() for _ in range(n_in)]\n",
    "        self.bias: float = 1.0\n",
    "\n",
    "    def __call__(self, y: list[float]) -> \"Units\":\n",
    "        if len(y) != len(self.w): \n",
    "            raise BaseException(\"Incorrect Dimensions\")\n",
    "\n",
    "        x_j: Scalar = sum((y_i * w_ji for y_i, w_ji in zip(y, self.w)))\n",
    "        out = x_j.sigmoid()\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        doc: str = \"Unit[\\n\"\n",
    "        for x_i in self.w:\n",
    "            doc += \"\\t\" + x_i.__str__() + \"\\n\"\n",
    "        doc += \"]\"\n",
    "        return doc\n",
    "\n",
    "class StateVector: # aka Tensor\n",
    "    \n",
    "    def __init__(self, n_in: int, n_out: int) -> None:\n",
    "        self.n_in: int = n_in\n",
    "        self.n_out: int = n_out\n",
    "        self.units: list[Units] = [Units(n_in) for _ in range(n_out)]\n",
    "\n",
    "    def __call__(self, y: list[float]) -> \"StateVector\":\n",
    "        out: list[Units] = [unit(y) for unit in self.units]\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        doc: str = f\"StateVector[{self.n_in}, {self.n_out}][\\n\"\n",
    "        for j in self.units:\n",
    "            doc += \"  \" + j.__str__() + \"\\n\"\n",
    "        doc += \"]\"\n",
    "        return doc\n",
    "\n",
    "class NeuralNetwork: \n",
    "    def __init__(self, n_in, layers: list[StateVector]) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.layers: list[StateVector] = layers\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "mlp = NeuralNetwork(3, [\n",
    "    StateVector(3, 10),\n",
    "    StateVector(10, 10),\n",
    "    StateVector(10, 1)\n",
    "])\n",
    "mlp([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
