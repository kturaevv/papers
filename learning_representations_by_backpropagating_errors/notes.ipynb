{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning representations by back-propagating errors\n",
    "\n",
    "---\n",
    "**Quick summary:** *paper that introduced and popularized the concept of backpropagation for Neural Networks*\n",
    "\n",
    "> The task is specified by giving the desired state vector of the output units for each state vector of the input units\n",
    "\n",
    "Quite convoluted way of saying that to make a network train we need to provide it the intput data with corresponding output we hope to achieve, i.e. a supervised learning. In other words, to map inputs to the outputs with the data.\n",
    "\n",
    "> ... state of the units in each layer are determined by applying equations (1) and (2) ...\n",
    "\n",
    "Description of how Neural Networks work, i.e. every output of a layer should correspond to every input node of next layer. And layers are pieced together by a linking linear function with non-linearity.\n",
    "\n",
    "A Unit consist of 2 values which is a **weight** and a **bias**. In a modern terminology this is reffered to as a *Neuron*\n",
    "\n",
    "> The total input, $x_{j}$ to unit j is a linear function of the outputs,\n",
    "> $y_{i}$ of the units that are connected to j and of the weights, $w_{ji}$\n",
    "> on these connections $ x_{j} = \\sum_{i}y{_i}w_{ji} $\n",
    "\n",
    "... the states of the \"units\", aka Neurons, are evaluated by the linear relation of inputs and a weight and applied non-linearity thereafter\n",
    "\n",
    "```python\n",
    "    def __call__(self, y: \"Units\") -> \"Units\":\n",
    "        x_j = sum(y_i * w_ji for y_i, w_ji in zip(y.w, self.w))\n",
    "        return x_j\n",
    "```\n",
    "\n",
    "Every Neuron accepts arbitrary number of inputs from previous layers and outputs a non-linearity for further use.\n",
    "\n",
    "Based on above mentioned ideas it is enough to \"build\" the basic implementation for a simplest, non-optimized neural network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(w=0.20481083816761766)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Units.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m a \u001b[38;5;241m=\u001b[39m Neuron() \u001b[38;5;241m*\u001b[39m Neuron()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mUnits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print(StateVector(1, 10) + StateVector(10, 10))\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Units.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Scalar:\n",
    "    \n",
    "    def __init__(self, data = None) -> None:\n",
    "        self.w: float = data if data is not None else random.uniform(-1, 1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Neuron(w={self.w})\"\n",
    "\n",
    "    def __add__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "\n",
    "        self.w += other.w\n",
    "        return self\n",
    "\n",
    "    def __mul__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        if not isinstance(other, Scalar):\n",
    "            other = Scalar(other)\n",
    "\n",
    "        self.w *= other.w\n",
    "        return self\n",
    "\n",
    "    def __pow__(self, power: int):\n",
    "        self.w ^= power\n",
    "        return self\n",
    "    \n",
    "    \"\"\" Handle other use cases using base operations. \"\"\"\n",
    "\n",
    "    def __neg__(self) -> \"Scalar\":  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other: \"Scalar\") -> \"Scalar\":  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other: \"Scalar\") -> \"Scalar\":  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other: \"Scalar\") -> \"Scalar\":  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other: \"Scalar\") -> \"Scalar\":  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other: \"Scalar\") -> \"Scalar\":  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other: \"Scalar\") -> \"Scalar\":  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "\n",
    "class Units: # aka a single Neuron\n",
    "    \n",
    "    def __init__(self, n_in: int) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.w: list[Scalar] = [Scalar() for _ in range(n_in)]\n",
    "        self.bias: float = 1.0\n",
    "\n",
    "    def __call__(self, y: \"Units\") -> \"Units\":\n",
    "        x_j = sum(y_i * w_ji for y_i, w_ji in zip(y.w, self.w))\n",
    "        return x_j\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        doc: str = \"Unit[\\n\"\n",
    "        for x_i in self.w:\n",
    "            doc += \"\\t\"\n",
    "            doc += x_i.__str__()\n",
    "            doc += \"\\n\"\n",
    "        doc += \"]\"\n",
    "        return doc\n",
    "\n",
    "class StateVector: # aka Tensor\n",
    "    pass\n",
    "\n",
    "class NeuralNetwork: \n",
    "    pass\n",
    "\n",
    "a = Scalar() * Scalar()\n",
    "print(a)\n",
    "print(Units(1, 10))\n",
    "# print(StateVector(1, 10) + StateVector(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A unit has a real-valued output, $y_{j}$ which is a non-linear function of its total input $ y_{j} = {1 \\over 1 + e^{-x_{j}} }$\n",
    "\n",
    "This is the non-linearity that makes the network learn stuff, which is commonly reffered to as *activation function*. The one mentioned in the paper is *Sigmoid activation function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
